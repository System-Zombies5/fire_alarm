{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTING LIBRARIES FOR CREATING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_preprocessing\n",
    "from keras_preprocessing import image\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATING DATASETS FOR TRAINING AND VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 999 images belonging to 2 classes.\n",
      "Found 999 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_datagen = ImageDataGenerator(rescale = 1./255,horizontal_flip=True,rotation_range=45,height_shift_range=0.2,fill_mode='nearest')\n",
    "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "train_generator = training_datagen.flow_from_directory(\"fire_dataset\",target_size=(224,224),class_mode='categorical',batch_size = 64)\n",
    "validation_generator = validation_datagen.flow_from_directory(\"fire_dataset\",target_size=(224,224),class_mode='categorical',batch_size= 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATING INCEPTIONV3 MODEL FOR IMAGE RECOGNITION AND CLASSIFICATION OF FIRE AND NONE FIRE IMAGE.\n",
    "OUR MODEL CONTAINS 1 INPUT LAYER(GLOBALAVERAGEPOOLING2D), 3 DENSE LAYERS AND 3 DROPOUT AYERS AND 1 OUTPUT LAYER. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "14/14 [==============================] - 39s 3s/step - loss: 6.0478 - acc: 0.6897 - val_loss: 0.3287 - val_acc: 0.7366\n",
      "Epoch 2/20\n",
      "14/14 [==============================] - 41s 3s/step - loss: 0.3046 - acc: 0.8634 - val_loss: 0.1648 - val_acc: 0.9464\n",
      "Epoch 3/20\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.3071 - acc: 0.8840 - val_loss: 0.1366 - val_acc: 0.9598\n",
      "Epoch 4/20\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.2253 - acc: 0.9242 - val_loss: 0.0907 - val_acc: 0.9777\n",
      "Epoch 5/20\n",
      "14/14 [==============================] - 43s 3s/step - loss: 0.2171 - acc: 0.9242 - val_loss: 0.1946 - val_acc: 0.9062\n",
      "Epoch 6/20\n",
      "14/14 [==============================] - 45s 3s/step - loss: 0.3661 - acc: 0.9036 - val_loss: 0.2093 - val_acc: 0.9509\n",
      "Epoch 7/20\n",
      "14/14 [==============================] - 45s 3s/step - loss: 0.1705 - acc: 0.9442 - val_loss: 0.1038 - val_acc: 0.9777\n",
      "Epoch 8/20\n",
      "14/14 [==============================] - 43s 3s/step - loss: 0.1427 - acc: 0.9449 - val_loss: 0.1293 - val_acc: 0.9598\n",
      "Epoch 9/20\n",
      "14/14 [==============================] - 43s 3s/step - loss: 0.1964 - acc: 0.9483 - val_loss: 0.0497 - val_acc: 0.9821\n",
      "Epoch 10/20\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.1541 - acc: 0.9483 - val_loss: 0.1026 - val_acc: 0.9554\n",
      "Epoch 11/20\n",
      "14/14 [==============================] - 43s 3s/step - loss: 0.1928 - acc: 0.9449 - val_loss: 0.0836 - val_acc: 0.9643\n",
      "Epoch 12/20\n",
      "14/14 [==============================] - 42s 3s/step - loss: 0.1584 - acc: 0.9460 - val_loss: 0.1215 - val_acc: 0.9509\n",
      "Epoch 13/20\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.1510 - acc: 0.9541 - val_loss: 0.1303 - val_acc: 0.9598\n",
      "Epoch 14/20\n",
      "14/14 [==============================] - 43s 3s/step - loss: 0.2260 - acc: 0.9323 - val_loss: 0.0534 - val_acc: 0.9821\n",
      "Epoch 15/20\n",
      "14/14 [==============================] - 43s 3s/step - loss: 0.2308 - acc: 0.9380 - val_loss: 0.1107 - val_acc: 0.9643\n",
      "Epoch 16/20\n",
      "14/14 [==============================] - 43s 3s/step - loss: 0.1356 - acc: 0.9575 - val_loss: 0.0779 - val_acc: 0.9643\n",
      "Epoch 17/20\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.1310 - acc: 0.9554 - val_loss: 0.0897 - val_acc: 0.9732\n",
      "Epoch 18/20\n",
      "14/14 [==============================] - 43s 3s/step - loss: 0.1035 - acc: 0.9701 - val_loss: 0.2401 - val_acc: 0.9464\n",
      "Epoch 19/20\n",
      "14/14 [==============================] - 43s 3s/step - loss: 0.1441 - acc: 0.9552 - val_loss: 0.0780 - val_acc: 0.9732\n",
      "Epoch 20/20\n",
      "14/14 [==============================] - 43s 3s/step - loss: 0.1758 - acc: 0.9495 - val_loss: 0.0588 - val_acc: 0.9777\n",
      "Epoch 1/10\n",
      "14/14 [==============================] - 55s 4s/step - loss: 0.5063 - acc: 0.7451 - val_loss: 0.0971 - val_acc: 0.9688\n",
      "Epoch 2/10\n",
      "14/14 [==============================] - 51s 4s/step - loss: 0.4531 - acc: 0.7635 - val_loss: 0.0945 - val_acc: 0.9643\n",
      "Epoch 3/10\n",
      "14/14 [==============================] - 51s 4s/step - loss: 0.3951 - acc: 0.8255 - val_loss: 0.0707 - val_acc: 0.9866\n",
      "Epoch 4/10\n",
      "14/14 [==============================] - 51s 4s/step - loss: 0.3450 - acc: 0.8519 - val_loss: 0.0909 - val_acc: 0.9777\n",
      "Epoch 5/10\n",
      "14/14 [==============================] - 52s 4s/step - loss: 0.2902 - acc: 0.8794 - val_loss: 0.1086 - val_acc: 0.9821\n",
      "Epoch 6/10\n",
      "14/14 [==============================] - 53s 4s/step - loss: 0.2628 - acc: 0.9047 - val_loss: 0.1070 - val_acc: 0.9688\n",
      "Epoch 7/10\n",
      "14/14 [==============================] - 54s 4s/step - loss: 0.2403 - acc: 0.9139 - val_loss: 0.1154 - val_acc: 0.9643\n",
      "Epoch 8/10\n",
      "14/14 [==============================] - 52s 4s/step - loss: 0.2328 - acc: 0.9219 - val_loss: 0.1375 - val_acc: 0.9598\n",
      "Epoch 9/10\n",
      "14/14 [==============================] - 52s 4s/step - loss: 0.2080 - acc: 0.9311 - val_loss: 0.1314 - val_acc: 0.9688\n",
      "Epoch 10/10\n",
      "14/14 [==============================] - 52s 4s/step - loss: 0.1857 - acc: 0.9346 - val_loss: 0.1484 - val_acc: 0.9554\n"
     ]
    }
   ],
   "source": [
    "input_tensor = Input(shape=(224, 224, 3))\n",
    "base_model = InceptionV3(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(2048, activation='relu')(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "predictions = Dense(2, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "for layer in base_model.layers:\n",
    "  layer.trainable = False\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(train_generator,steps_per_epoch = 14,epochs = 20,validation_data = validation_generator,validation_steps = 14)\n",
    "\n",
    "#To train the top 2 inception blocks, freeze the first 249 layers and unfreeze the rest.\n",
    "for layer in model.layers[:249]:\n",
    "  layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "  layer.trainable = True\n",
    "#Recompile the model for these modifications to take effect\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(learning_rate=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(train_generator,steps_per_epoch = 14,epochs = 10,validation_data = validation_generator,validation_steps = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APPLICATION OF OUR MODEL ON LIVE FEED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from playsound import playsound\n",
    "from keras.preprocessing import image\n",
    "#Load the saved model\n",
    "video = cv2.VideoCapture(0)\n",
    "for i in range(100):\n",
    "        _,frame = video.read()\n",
    "#Convert the captured frame into RGB\n",
    "        im = Image.fromarray(frame,'RGB')\n",
    "#Resizing into 224x224 because we trained the model with this image size.\n",
    "        im = im.resize((224,224))\n",
    "        img_array = image.img_to_array(im)\n",
    "        img_array = np.expand_dims(img_array, axis=0) / 255\n",
    "        probabilities = model.predict(img_array)[0]\n",
    "        #Calling the predict method on model to predict 'fire' on the image\n",
    "        prediction = np.argmax(probabilities)\n",
    "        #if prediction is 0, which means there is fire in the frame.\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        #if prediction == 0:\n",
    "                \n",
    "                #playsound('Fire alarm - 2018.mp3')\n",
    "                #print('playing sound using  playsound')\n",
    "        print(prediction)\n",
    "        cv2.imshow(\"Capturing\", frame)\n",
    "        key=cv2.waitKey(1)\n",
    "        if key == ord('q'):\n",
    "                break\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e1e75ddac2269c08fe69e3ddbb1cbe2523492437ad185b520d28cbde1c51c81f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
